{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recreating digest2 output for all the XML files \n",
    "We want to recreate the digest2 output for all the XML files from the NEO Surveyor mission. The digest2 score needs to contain all the available values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up files and directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import shutil\n",
    "import subprocess\n",
    "import time\n",
    "import os\n",
    "\n",
    "import xmltodict\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing the XML files\n",
    "xml_dir = \"NEOSurveyordata-20241021/\"\n",
    "xml_files = [\n",
    "    \"2024-10-21T16_54_54.398_000000R8.xml\",\n",
    "    \"2024-10-21T16_56_11.525_000000R9.xml\",\n",
    "]\n",
    "\n",
    "# Off-ecliptic and on-ecliptic XML files\n",
    "off_ecliptic_file = xml_dir+\"off_ecl_tracklet2desig3.dat\"\n",
    "on_ecliptic_file = xml_dir+\"on_ecl_tracklet2desig3.dat\"\n",
    "\n",
    "# Digest2 executable\n",
    "digest2_exec = \"digest2/digest2/digest2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUTINES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyze each XML file\n",
    "def analyze_xml(xml_file):\n",
    "    \"\"\" Analyze the content of the XML file. \"\"\"\n",
    "    with open(xml_file, 'r') as file:\n",
    "        xml_string = file.read()\n",
    "\n",
    "    # Convert the XML string to a Python dictionary\n",
    "    xml_dict = xmltodict.parse(xml_string)\n",
    "        \n",
    "    # Convert the dictionary to a DataFrame\n",
    "    xml_df = pd.DataFrame(xml_dict['ades']['obsBlock']['obsData']['optical'])\n",
    "    \n",
    "    # Count the number of observations\n",
    "    print(f'{xml_df.shape[0]}  observations in  {xml_file}')\n",
    "    # Count the number of unique trksubs\n",
    "    print(f'{xml_df[\"trkSub\"].nunique()}  unique trksubs in  {xml_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run digest2 code on single file\n",
    "def run_digest2(file_path):\n",
    "    # Check if the file exists\n",
    "    if not os.path.isfile(file_path):\n",
    "        print(f\"File {file_path} does not exist.\")\n",
    "        return False\n",
    "    print(f\"Processing: {file_path}\")\n",
    "\n",
    "    start_time = time.time() # starting a timer\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "        f\"{digest2_exec} -c MPC.config {file_path}\",\n",
    "        capture_output=True, text=True, check=True, shell=True\n",
    "        )\n",
    "        if result.stderr:\n",
    "            print(f\"Error: {result.stderr}\")\n",
    "\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        print(f\"Command output: {e.output}\")\n",
    "        return False\n",
    "\n",
    "    end_time = time.time() # ending the timer\n",
    "    elapsed_time = end_time - start_time\n",
    "\n",
    "    print(f\"Time taken for {file_path}: {elapsed_time:.4f} seconds\\n\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match trksubs with designations from NEO Surveyor \n",
    "def match_desig_trksub(offecliptic_file:str, onecliptic_file:str) -> dict:\n",
    "    \"\"\" Create a dictionary that matches the trksubs to the designations given by the NEO Surveyor Team \"\"\"\n",
    "\n",
    "    file_list = [offecliptic_file, onecliptic_file]\n",
    "    # Empty dictionary\n",
    "    trksub_desig = {}\n",
    "    \n",
    "    # Count the number of NEAs and not NEAs in the file\n",
    "    nnea = 0\n",
    "    nnonnea = 0\n",
    "    for f in file_list:\n",
    "        # Check if the file exists\n",
    "        if not os.path.isfile(f):\n",
    "            print(f\"File {f} not found.\")\n",
    "            return None\n",
    "        # Open the file \n",
    "        with open(f, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                # Create a dictionary to store the matches\n",
    "                sp = line.split()\n",
    "                trksub = sp[0]\n",
    "                desig = sp[1]\n",
    "                # NEA case\n",
    "                if desig[3] == \"0\":\n",
    "                    NEA = '0'\n",
    "                    nnea += 1\n",
    "                else:\n",
    "                    # MBA case\n",
    "                    NEA = '1'\n",
    "                    nnonnea += 1\n",
    "                trksub_desig[trksub] = [desig, NEA]\n",
    "                \n",
    "    # Print the number of NEAs and not NEAs\n",
    "    print(f\"Total number of NEAs in the off-ecliptic and on-ecliptic data: {nnea}\")\n",
    "    print(f\"Total number of non-NEAs in the off-ecliptic and on-ecliptic data: {nnonnea}\")\n",
    "\n",
    "    return trksub_desig\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the output needed by the machine learning code \n",
    "def create_output_format(xml_file: str, digest2_result:list, trksub_desig : dict) -> None:\n",
    "    \"\"\" \n",
    "    Create the output format needed by the machine learning code.\n",
    "    \"\"\"\n",
    "    # Create output file \n",
    "    filtering_output = open(xml_file.replace(\".xml\",\".digest2_filter\"), \"w\", encoding='utf-8')\n",
    "    ml_output = open(xml_file.replace(\".xml\",\".digest2_ml\"), \"w\", encoding='utf-8')\n",
    "    output = open(xml_file.replace(\".xml\",\".digest2\"), \"w\", encoding='utf-8')\n",
    "    # Write header\n",
    "    output.write(\"trksub,Int1,Int2,Neo1,Neo2,MC1,MC2,Hun1,Hun2,Pho1,Pho2,MB1_1,MB1_2,Pal1,Pal2,Han1,Han2,MB2_1,MB2_2,MB3_1,MB3_2,Hil1,Hil2,JTr1,JTr2,JFC1,JFC2\\n\")\n",
    "    # If filtering is true, add the NEA class to the header and call it \"class\"\n",
    "    filtering_output.write(\"trksub,Int1,Int2,Neo1,Neo2,MC1,MC2,Hun1,Hun2,Pho1,Pho2,MB1_1,MB1_2,Pal1,Pal2,Han1,Han2,MB2_1,MB2_2,MB3_1,MB3_2,Hil1,Hil2,JTr1,JTr2,JFC1,JFC2,class\\n\")\n",
    "    # If machine learning is true, add the NEA class to the header and call it \"orbtype\"\n",
    "    ml_output.write(\"trksub,Int1,Int2,Neo1,Neo2,MC1,MC2,Hun1,Hun2,Pho1,Pho2,MB1_1,MB1_2,Pal1,Pal2,Han1,Han2,MB2_1,MB2_2,MB3_1,MB3_2,Hil1,Hil2,JTr1,JTr2,JFC1,JFC2,orbtype\\n\")\n",
    "        \n",
    "    # Count the number of trksubs processed\n",
    "    ntrksub = 0\n",
    "    # Count the number of matched trksubs\n",
    "    nmatched = 0\n",
    "    nnea = 0\n",
    "    nmba = 0\n",
    "    \n",
    "    # Read digest2 results\n",
    "    for line in digest2_result.splitlines()[2:]:\n",
    "        # Split line into columns\n",
    "        sp = line.split()\n",
    "\n",
    "        # Increment the number of trksubs processed\n",
    "        ntrksub += 1\n",
    "        \n",
    "        # Always write the output file for digest2\n",
    "        output.write(','.join(sp) + \"\\n\")\n",
    "        \n",
    "        # Find if the object is an NEA or not\n",
    "        if sp[0] not in trksub_desig:\n",
    "            # If the object is not in the dictionary\n",
    "            continue\n",
    "        \n",
    "        # Increment the number of matched trksubs\n",
    "        nmatched += 1\n",
    "        NEAclass = trksub_desig[sp[0]][1]\n",
    "        if NEAclass == '0':\n",
    "            nnea += 1\n",
    "        else:\n",
    "            nmba += 1\n",
    "        output_string = ','.join(sp) + ',' + NEAclass + \"\\n\"\n",
    "        # Write to filtering output file\n",
    "        filtering_output.write(output_string)\n",
    "        # Write to machine learning output file\n",
    "        ml_output.write(output_string)\n",
    "        \n",
    "    # Close output file\n",
    "    output.close()\n",
    "    filtering_output.close()\n",
    "    ml_output.close()\n",
    "    \n",
    "    print(f\"Processed {ntrksub} trksubs, matched {nmatched} trksubs in {xml_file}.\")\n",
    "    print(f\"Found {nnea} NEAs and {nmba} MBAs in {xml_file}.\")\n",
    "\n",
    "    print(f\"Output file {xml_file.replace('.xml','.digest2')} created.\")\n",
    "    print(f\"Output file {xml_file.replace('.xml','.digest2_filter')} created.\")\n",
    "    print(f\"Output file {xml_file.replace('.xml','.digest2_ml')} created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_digest2_output(xml_dir: str, xml_files: list, trksub_desig: dict) -> None:\n",
    "    \n",
    "    # Run digest2 on single files\n",
    "    for xml_file in xml_files:\n",
    "        \n",
    "        # Analyze the XML file\n",
    "        print(\" ------ Analyze XML file ------ \")\n",
    "        analyze_xml(os.path.join(xml_dir, xml_file))\n",
    "\n",
    "        # Run digest2 on the XML file\n",
    "        print(\" ------ Run digest2 on XML file ------ \")\n",
    "        digest2_result = run_digest2(os.path.join(xml_dir, xml_file))\n",
    "        if not digest2_result:\n",
    "            print(f\"Digest2 failed for {xml_file}. Skipping further processing.\")\n",
    "            continue\n",
    "        \n",
    "        if not trksub_desig:\n",
    "            print(\"Failed to match trksubs with designations. Skipping further processing.\")\n",
    "            continue\n",
    "        \n",
    "        # Process the results\n",
    "        print(\" ------ Create output format ------ \")\n",
    "        create_output_format(xml_file, digest2_result.stdout, trksub_desig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze output from digest2\n",
    "def analyze_digest2_output(digest2_file: str, digest2_thresh : int = 65) -> None:\n",
    "    \"\"\" Analyze the output from digest2. \"\"\"\n",
    "\n",
    "    output_df = pd.read_csv(digest2_file)\n",
    "    \n",
    "    # Count the number of NEAs and non-NEAs\n",
    "    nea_count = output_df[output_df['Neo2'] >= digest2_thresh].shape[0]\n",
    "    nonnea_count = output_df[output_df['Neo2'] < digest2_thresh].shape[0]\n",
    "\n",
    "    print(f\"Number of NEAs in {digest2_file}: {nea_count}\")\n",
    "    print(f\"Number of non-NEAs in {digest2_file}: {nonnea_count}\")\n",
    "    \n",
    "# Analyze missing NEAs from digest2\n",
    "def analyze_missing_NEAs(digest2_file: str, digest2_thresh : int = 65) -> None:\n",
    "    \"\"\" Analyze the output from digest2. \"\"\"\n",
    "\n",
    "    output_df = pd.read_csv(digest2_file)\n",
    "    \n",
    "    # Check NEAs in NON_NEA df\n",
    "    notnea_df = output_df[output_df['Neo2'] < digest2_thresh]\n",
    "    misidentified_nea = notnea_df[notnea_df['class'] == 0]\n",
    "    print(f\"Number of misidentified NEAs in {digest2_file}: {misidentified_nea.shape[0]}\")\n",
    "    #print(misidentified_nea)\n",
    "    \n",
    "    # Check for non-NEAs in NEA df\n",
    "    nea_df = output_df[output_df['Neo2'] >= digest2_thresh]\n",
    "    misidentified_nonnea = nea_df[nea_df['class'] == 1]\n",
    "    print(f\"Number of misidentified non-NEAs in {digest2_file}: {misidentified_nonnea.shape[0]}\")\n",
    "    #print(misidentified_nonnea)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_optimal_thresholds(optimal_thresh_file: str, digest2_file: str) -> None:\n",
    "    \"\"\" Analyze the optimal thresholds from digest2. \"\"\"\n",
    "\n",
    "    # The file only contains the trksubs that are not NEAs\n",
    "    thresh_df = pd.read_csv(optimal_thresh_file, header=None, names=['trksub'])\n",
    "    print(f\"Number of trksubs in {optimal_thresh_file}: {thresh_df.shape[0]}\")\n",
    "    digest2_df = pd.read_csv(digest2_file)\n",
    "    \n",
    "    # Count the number of NEAs and non-NEAs\n",
    "    neas = digest2_df[digest2_df[\"class\"] == 0]\n",
    "    # We want to check if there are NEAs in the optimal thresholds file\n",
    "    nea_count = neas[neas['trksub'].isin(thresh_df['trksub'])].shape[0]\n",
    "    print(f\"Number of NEAs in {optimal_thresh_file}: {nea_count}\")\n",
    "    #print(neas[neas['trksub'].isin(thresh_df['trksub'])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Match trksubs with designations ------ \n",
      "Total number of NEAs in the off-ecliptic and on-ecliptic data: 1280\n",
      "Total number of non-NEAs in the off-ecliptic and on-ecliptic data: 550120\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Main execution flow \n",
    "1. Create a dictionary that matches the trksubs to the designations given by the NEO Surveyor Team\n",
    "2. Analyze each XML file\n",
    "3. Run digest2 on each XML file\n",
    "4. Create the output format needed by the filtering and the machine learning code\n",
    "\"\"\"\n",
    "\n",
    "# Match trksubs with designations\n",
    "print(\" ------ Match trksubs with designations ------ \")\n",
    "trksub_desig = match_desig_trksub(off_ecliptic_file, on_ecliptic_file)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Create digest2 output format for each XML file ------ \n",
      " ------ Analyze XML file ------ \n",
      "2522  observations in  NEOSurveyordata-20241021/2024-10-21T16_54_54.398_000000R8.xml\n",
      "558  unique trksubs in  NEOSurveyordata-20241021/2024-10-21T16_54_54.398_000000R8.xml\n",
      " ------ Run digest2 on XML file ------ \n",
      "Processing: NEOSurveyordata-20241021/2024-10-21T16_54_54.398_000000R8.xml\n",
      "Time taken for NEOSurveyordata-20241021/2024-10-21T16_54_54.398_000000R8.xml: 24.7285 seconds\n",
      "\n",
      " ------ Create output format ------ \n",
      "Processed 558 trksubs, matched 454 trksubs in 2024-10-21T16_54_54.398_000000R8.xml.\n",
      "Found 49 NEAs and 405 MBAs in 2024-10-21T16_54_54.398_000000R8.xml.\n",
      "Output file 2024-10-21T16_54_54.398_000000R8.digest2 created.\n",
      "Output file 2024-10-21T16_54_54.398_000000R8.digest2_filter created.\n",
      "Output file 2024-10-21T16_54_54.398_000000R8.digest2_ml created.\n",
      " ------ Analyze XML file ------ \n",
      "9335  observations in  NEOSurveyordata-20241021/2024-10-21T16_56_11.525_000000R9.xml\n",
      "2032  unique trksubs in  NEOSurveyordata-20241021/2024-10-21T16_56_11.525_000000R9.xml\n",
      " ------ Run digest2 on XML file ------ \n",
      "Processing: NEOSurveyordata-20241021/2024-10-21T16_56_11.525_000000R9.xml\n",
      "Time taken for NEOSurveyordata-20241021/2024-10-21T16_56_11.525_000000R9.xml: 109.8252 seconds\n",
      "\n",
      " ------ Create output format ------ \n",
      "Processed 2032 trksubs, matched 1938 trksubs in 2024-10-21T16_56_11.525_000000R9.xml.\n",
      "Found 65 NEAs and 1873 MBAs in 2024-10-21T16_56_11.525_000000R9.xml.\n",
      "Output file 2024-10-21T16_56_11.525_000000R9.digest2 created.\n",
      "Output file 2024-10-21T16_56_11.525_000000R9.digest2_filter created.\n",
      "Output file 2024-10-21T16_56_11.525_000000R9.digest2_ml created.\n"
     ]
    }
   ],
   "source": [
    "print(\" ------ Create digest2 output format for each XML file ------ \")\n",
    "create_digest2_output(xml_dir, xml_files, trksub_desig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ------ Analyze output files ------ \n",
      "Number of NEAs in 2024-10-21T16_54_54.398_000000R8.digest2_filter: 36\n",
      "Number of non-NEAs in 2024-10-21T16_54_54.398_000000R8.digest2_filter: 418\n",
      " ------ Analyze output files ------ \n",
      "Number of NEAs in 2024-10-21T16_56_11.525_000000R9.digest2_filter: 40\n",
      "Number of non-NEAs in 2024-10-21T16_56_11.525_000000R9.digest2_filter: 1898\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Main execution flow \n",
    "5. Analyze the output files \n",
    "\"\"\"\n",
    "# Analyze the output files\n",
    "for xml_file in xml_files:\n",
    "    print(\" ------ Analyze output files ------ \")\n",
    "    #analyze_digest2_output(xml_file.replace(\".xml\", \".digest2\"),digest2_thresh=90)\n",
    "    analyze_digest2_output(xml_file.replace(\".xml\", \".digest2_filter\"), digest2_thresh=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing the digest2 score for the missing NEAs\n",
    "We noticed that if we set digest2=100, we are then missing some NEAs (between 60% and 74% of objects are actually correctly identified) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the output files\n",
    "for xml_file in xml_files:\n",
    "    print(\" ------ Analyze missing NEAs ------ \")\n",
    "    analyze_missing_NEAs(xml_file.replace(\".xml\", \".digest2_filter\"), digest2_thresh=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Optimal threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import code from github repository\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/meilin/Desktop/NEOSurveyor/ML_digest2_methods-main/src\")\n",
    "\n",
    "from neocp_filter import filter_and_output_passed_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for xml in xml_files:\n",
    "    print(f\"Processing file: {xml}\")\n",
    "    input_csv = xml.replace(\".xml\", \".digest2_filter\")\n",
    "    threshold_json = f\"/Users/meilin/Desktop/NEOSurveyor/ML_digest2_methods-main/src/optimal_thresholds.json\"\n",
    "    output_csv = input_csv.replace(\".digest2_filter\", \"_filter_passed.csv\")\n",
    "\n",
    "    filter_and_output_passed_entries(input_csv, threshold_json, output_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze optimal thresholds output\n",
    "for xml in xml_files:\n",
    "    optimal_thresh_file = xml.replace(\".xml\", \"_filter_passed.csv\")\n",
    "    digest2_file = xml.replace(\".xml\", \".digest2_filter\")\n",
    "    print(f\"Analyzing optimal thresholds for {optimal_thresh_file} and {digest2_file}\")\n",
    "    analyze_optimal_thresholds(optimal_thresh_file, digest2_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Running the machine learning code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_python = \"ML_digest2_methods-main/src/testing_pipeline.py\"\n",
    "model_dir = \"ML_digest2_methods-main/models/\"\n",
    "for xml in xml_files:\n",
    "    print(f\"Processing file: {xml}\")\n",
    "    input_csv = xml.replace(\".xml\", \".digest2_ml\")\n",
    "    # Run the machine learning code\n",
    "    subprocess.run(\n",
    "        f\"python3 {ml_python} --model_dir {model_dir} --test_data {input_csv} --save_results\",\n",
    "        shell=True, check=True\n",
    "    )\n",
    "    # Copy the results to the current directory\n",
    "    shutil.copyfile(f\"ML_digest2_methods-main/models/GBM_predictions.csv\", f\"{xml.replace('.xml', '_GBM_predictions.csv')}\")\n",
    "    shutil.copyfile(f\"ML_digest2_methods-main/models/confusion_matrices.png\", f\"{xml.replace('.xml', '_confusion_matrices.png')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
